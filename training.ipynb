{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL based Recommendation System\n",
    "\n",
    "### This is part 2: Environment Setup & Training\n",
    "In this part we will:\n",
    "\n",
    "1. Create a training environment\n",
    "2. Create a training agent\n",
    "3. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the preprocessed data\n",
    "<p>We will use the preprocessed data from the previous notebook, which we had saved in a pickle file.<br> We will also extract the key metrics, such as the number of unique users and items, from the preprocessed data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 192403\n",
      "Number of items: 62993\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "df_full = pd.read_pickle('df_full.pkl')\n",
    "df_train = df_full[df_full['set'] == 'train']\n",
    "\n",
    "# Calculate number of unique users and items\n",
    "num_users = df_train['user_idx'].nunique()\n",
    "num_items = df_train['item_idx'].nunique()\n",
    "print(f\"Number of users: {num_users}\")\n",
    "print(f\"Number of items: {num_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare User Interaction Data\n",
    "<p>In this step, we will prepare the user interaction data for training the model.<br> We shall compute the following</p>\n",
    "\n",
    "1. `user_interactions`: Dict mapping user_idx to set of item_idx that user has interacted with\n",
    "2. `user_ratings`: Dict mapping (user_idx, item_idx) touples to corresponding rating  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user_interactions: {user_idx: set of item_idx}\n",
    "user_interactions = df_train.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
    "\n",
    "# Create user_ratings: {(user_idx, item_idx): rating}\n",
    "user_ratings = df_train.set_index(['user_idx', 'item_idx'])['overall'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set up the training environment class\n",
    "<p>We will set up a class <code>AmazonEnv</code> that inherits from <code>gym.Env</code> and implements the <code>step</code> and <code>reset</code> methods. <br>The <code>step</code> method will take the user's action and return the reward, the next state, and whether the episode is done.</p>\n",
    "<p>We will initialize the class with training data, history length <code>N</code> and episode length <code>M</code>. We will also set up the <code>action_space</code> and <code>observation_space</code> attributes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonEnv(gym.Env):\n",
    "    def __init__(self, df_train, N=5, M=10):\n",
    "        super(AmazonEnv, self).__init__()\n",
    "        self.df_train = df_train\n",
    "        self.user_interactions = user_interactions\n",
    "        self.user_ratings = user_ratings\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.N = N      #Length of history in state\n",
    "        self.M = M      #Maximum steps per episode\n",
    "        self.current_user = None\n",
    "        self.history = []   #List of (item_idx, reward) tuples\n",
    "        \n",
    "        #Observation space: [user_idx, item1, ..., itemN, rating1, ..., ratingN]\n",
    "        high = np.array([num_users - 1] + [num_items - 1] * N + [5] * N, dtype=np.float32) #Because we have num_users total users, num_items total items and 5 possible ratings\n",
    "        self.observation_space = spaces.Box(low=0, high=high, shape=(1 + 2 * N,), dtype=np.float32)\n",
    "        \n",
    "        # Action space: Recommend any item\n",
    "        self.action_space = spaces.Discrete(self.num_items) #Because we have the number of possible actions as the number of items\n",
    "        \n",
    "    #Implement reset method\n",
    "    def reset(self):\n",
    "        \"\"\"This method initializes the environment for a new episode\n",
    "        1: Randomly selects a user\n",
    "        2: Clear their reccomendation history\n",
    "        3: Return an initial state vector with the user index and zeros for the history.\n",
    "        \n",
    "        An empty history simulated the start of a reccomendation sequence\n",
    "        \"\"\"\n",
    "        #Choose a random user\n",
    "        self.current_user = np.random.choice(self.df_train['user_idx'].unique())\n",
    "        self.history = []\n",
    "        #Initial state: [user_idx, 0, ..., 0]\n",
    "        state = np.array([self.current_user] + [0] * self.N + [0] * self.N, dtype=np.float32)\n",
    "        return state\n",
    "    \n",
    "    #Implement step method\n",
    "    def step(self, action):\n",
    "        \"\"\"Define the environment's response to an agent's action (item recommendation).\n",
    "        1. Check if the recommended item (action) is in the user's interaction set\n",
    "        2. If yes, set reward to the rating from user_ratings (The reward reflects the quality of the recommendation based on historical data.)\n",
    "        3. If no, reward is 0\n",
    "        4. Append the item and reward to the history\n",
    "        5. Update the state with the last N items and rewards, padding with zeros if the history is shorter than N.\n",
    "        6. Set done to True if the episode reaches M steps\n",
    "        \"\"\"\n",
    "        #Compute the reward based on user's interaction history\n",
    "        if (self.current_user, action) in self.user_ratings:\n",
    "            reward = self.user_ratings[(self.current_user, action)]\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        \n",
    "        #Update the history with the reccomendation\n",
    "        self.history.append((action, reward))\n",
    "        \n",
    "        #Extract the last N items and ratings from the history\n",
    "        if len(self.history) < self.N: #We apply padding to the history if it is not long enough\n",
    "            state_items = [0] * (self.N - len(self.history)) + [item for item, _ in self.history]\n",
    "            state_ratings = [0] * (self.N - len(self.history)) + [rating for _, rating in self.history]\n",
    "        else:\n",
    "            state_items = [item for item, _ in self.history[-self.N:]]\n",
    "            state_ratings = [rating for _, rating in self.history[-self.N:]]\n",
    "            \n",
    "        #Construct the state vector\n",
    "        state = np.array([self.current_user] + state_items + state_ratings, dtype=np.float32)\n",
    "        \n",
    "        #Check if the episode is done\n",
    "        done = len(self.history) >= self.M\n",
    "        \n",
    "        return state, reward, done, {}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (Optional): Test the environment\n",
    "<p>Ensure the environment functions correctly before training the agent.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: [184761.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.]\n",
      "State: [184761.      0.      0.      0.      0.  43354.      0.      0.      0.\n",
      "      0.      0.], Reward: 0.0, Done: False\n",
      "State: [184761.      0.      0.      0.  43354.  58598.      0.      0.      0.\n",
      "      0.      0.], Reward: 0.0, Done: False\n",
      "State: [184761.      0.      0.  43354.  58598.  24137.      0.      0.      0.\n",
      "      0.      0.], Reward: 0.0, Done: False\n",
      "State: [184761.      0.  43354.  58598.  24137.   4957.      0.      0.      0.\n",
      "      0.      0.], Reward: 0.0, Done: False\n",
      "State: [184761.  43354.  58598.  24137.   4957.  24293.      0.      0.      0.\n",
      "      0.      0.], Reward: 0.0, Done: False\n",
      "State: [184761.  58598.  24137.   4957.  24293.  31586.      0.      0.      0.\n",
      "      0.      0.], Reward: 0.0, Done: False\n",
      "State: [184761.  24137.   4957.  24293.  31586.  47172.      0.      0.      0.\n",
      "      0.      0.], Reward: 0.0, Done: False\n",
      "State: [184761.   4957.  24293.  31586.  47172.  33396.      0.      0.      0.\n",
      "      0.      0.], Reward: 0.0, Done: False\n",
      "State: [184761.  24293.  31586.  47172.  33396.    401.      0.      0.      0.\n",
      "      0.      0.], Reward: 0.0, Done: False\n",
      "State: [184761.  31586.  47172.  33396.    401.  26064.      0.      0.      0.\n",
      "      0.      0.], Reward: 0.0, Done: True\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and test the environment\n",
    "env = AmazonEnv(df_train, N=5, M=10)\n",
    "state = env.reset()\n",
    "print(\"Initial state:\", state)\n",
    "\n",
    "# Run a few steps with random actions\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # Random item recommendation\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(f\"State: {state}, Reward: {reward}, Done: {done}\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The environment seems to be working correctly, and is now ready. Let's move on to the training process.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
